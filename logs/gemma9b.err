/home/joberant/NLP_2425a/sharonsaban/anaconda3/envs/sharon_env/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/joberant/NLP_2425a/sharonsaban/anaconda3/envs/sharon_env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
/home/joberant/NLP_2425a/sharonsaban/anaconda3/envs/sharon_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Fetching 8 files:   0%|          | 0/8 [09:01<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [02:09<15:06, 129.53s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [05:28<17:01, 170.22s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [08:49<15:21, 184.22s/it]Loading checkpoint shards:  50%|█████     | 4/8 [11:29<11:39, 174.76s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [14:42<09:04, 181.46s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [17:54<06:10, 185.00s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [21:13<03:09, 189.64s/it]Loading checkpoint shards: 100%|██████████| 8/8 [22:35<00:00, 155.43s/it]Loading checkpoint shards: 100%|██████████| 8/8 [22:35<00:00, 169.50s/it]
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/home/joberant/NLP_2425a/sharonsaban/anaconda3/envs/sharon_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/joberant/NLP_2425a/sharonsaban/anaconda3/envs/sharon_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
slurmstepd: error: _cgroup_procs_check: failed on path (null)/cgroup.procs: No such file or directory
slurmstepd: error: Cannot write to cgroup.procs for (null)
slurmstepd: error: Unable to move pid 1573373 to init root cgroup (null)
