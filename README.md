NLP_FINAL_PROJECT
This project explores the impact of various transformer layer duplication strategies on the performance of Gemma and LLaMA models, particularly focusing on multi-hop reasoning tasks such as those found in the MuSiQue dataset.

üìÅ Repository Structure
data_sets/: Contains the datasets used for training and evaluation, including MuSiQue and MMLU.

eval/: Scripts and tools for evaluating model performance across different configurations.

logs/: Stores training and evaluation logs for reproducibility and analysis.

run_exp.py: Main script to execute experiments with specified configurations.

run_exp_dup_chunks.py: Script to run experiments involving duplication of specific chunks within the model architecture.
GitHub
