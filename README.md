NLP_FINAL_PROJECT
This project explores the impact of various transformer layer duplication strategies on the performance of Gemma and LLaMA models, particularly focusing on 3 datasets

üìÅ Repository Structure
data_sets/: Contains the datasets used for training and evaluation, including MuSiQue, DROP and MMLU.

eval/: Scripts and tools for evaluating model performance across different configurations.

logs/: Stores training and evaluation logs for reproducibility and analysis.

run_exp.py: Main script to execute experiments with specified configurations.

run_exp_dup_chunks.py: Script to run experiments involving duplication of specific chunks within the model architecture.
GitHub
