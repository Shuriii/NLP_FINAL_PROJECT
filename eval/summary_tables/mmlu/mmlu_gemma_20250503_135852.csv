model,configuration,position,technique,size,continuous,accuracy,total,correct,accuracy_percent,per_subject,per_super_category,% improvement
gemma-2-9b,original,,,,,0.72,500,360,72.0,"{'professional_law': {'accuracy': 0.5818181818181818, 'total': 55, 'correct': 32, 'accuracy_percent': 58.18}, 'moral_scenarios': {'accuracy': 0.40625, 'total': 32, 'correct': 13, 'accuracy_percent': 40.62}, 'miscellaneous': {'accuracy': 0.8571428571428571, 'total': 28, 'correct': 24, 'accuracy_percent': 85.71}, 'professional_psychology': {'accuracy': 0.7727272727272727, 'total': 22, 'correct': 17, 'accuracy_percent': 77.27}, 'high_school_psychology': {'accuracy': 0.9473684210526315, 'total': 19, 'correct': 18, 'accuracy_percent': 94.74}, 'high_school_macroeconomics': {'accuracy': 0.6428571428571429, 'total': 14, 'correct': 9, 'accuracy_percent': 64.29}, 'elementary_mathematics': {'accuracy': 0.5384615384615384, 'total': 13, 'correct': 7, 'accuracy_percent': 53.85}, 'moral_disputes': {'accuracy': 0.8333333333333334, 'total': 12, 'correct': 10, 'accuracy_percent': 83.33}, 'prehistory': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'philosophy': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'high_school_biology': {'accuracy': 0.9090909090909091, 'total': 11, 'correct': 10, 'accuracy_percent': 90.91}, 'nutrition': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'professional_accounting': {'accuracy': 0.9, 'total': 10, 'correct': 9, 'accuracy_percent': 90.0}, 'professional_medicine': {'accuracy': 0.9, 'total': 10, 'correct': 9, 'accuracy_percent': 90.0}, 'high_school_mathematics': {'accuracy': 0.8, 'total': 10, 'correct': 8, 'accuracy_percent': 80.0}, 'clinical_knowledge': {'accuracy': 0.7777777777777778, 'total': 9, 'correct': 7, 'accuracy_percent': 77.78}, 'security_studies': {'accuracy': 0.6666666666666666, 'total': 9, 'correct': 6, 'accuracy_percent': 66.67}, 'high_school_microeconomics': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_world_history': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'conceptual_physics': {'accuracy': 1.0, 'total': 8, 'correct': 8, 'accuracy_percent': 100.0}, 'marketing': {'accuracy': 1.0, 'total': 8, 'correct': 8, 'accuracy_percent': 100.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_us_history': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_chemistry': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'sociology': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_geography': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'world_religions': {'accuracy': 1.0, 'total': 6, 'correct': 6, 'accuracy_percent': 100.0}, 'virology': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'high_school_european_history': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'high_school_physics': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'electrical_engineering': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'college_biology': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'anatomy': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'human_sexuality': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'public_relations': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'jurisprudence': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'management': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_physics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'business_ethics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'college_chemistry': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_computer_science': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_mathematics': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}, 'computer_security': {'accuracy': 0.6666666666666666, 'total': 3, 'correct': 2, 'accuracy_percent': 66.67}}","{'Humanities': {'accuracy': 0.6385542168674698, 'total': 166, 'correct': 106, 'accuracy_percent': 63.86}, 'Other': {'accuracy': 0.7410714285714286, 'total': 112, 'correct': 83, 'accuracy_percent': 74.11}, 'Social_Sciences': {'accuracy': 0.8090909090909091, 'total': 110, 'correct': 89, 'accuracy_percent': 80.91}, 'STEM': {'accuracy': 0.7321428571428571, 'total': 112, 'correct': 82, 'accuracy_percent': 73.21}}",29.03
gemma-2-2b,original,,,,,0.558,500,279,55.8,"{'professional_law': {'accuracy': 0.41818181818181815, 'total': 55, 'correct': 23, 'accuracy_percent': 41.82}, 'moral_scenarios': {'accuracy': 0.28125, 'total': 32, 'correct': 9, 'accuracy_percent': 28.12}, 'miscellaneous': {'accuracy': 0.7142857142857143, 'total': 28, 'correct': 20, 'accuracy_percent': 71.43}, 'professional_psychology': {'accuracy': 0.45454545454545453, 'total': 22, 'correct': 10, 'accuracy_percent': 45.45}, 'high_school_psychology': {'accuracy': 0.7894736842105263, 'total': 19, 'correct': 15, 'accuracy_percent': 78.95}, 'high_school_macroeconomics': {'accuracy': 0.42857142857142855, 'total': 14, 'correct': 6, 'accuracy_percent': 42.86}, 'elementary_mathematics': {'accuracy': 0.3076923076923077, 'total': 13, 'correct': 4, 'accuracy_percent': 30.77}, 'moral_disputes': {'accuracy': 0.5, 'total': 12, 'correct': 6, 'accuracy_percent': 50.0}, 'prehistory': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'philosophy': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'high_school_biology': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'nutrition': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'professional_accounting': {'accuracy': 0.7, 'total': 10, 'correct': 7, 'accuracy_percent': 70.0}, 'professional_medicine': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'high_school_mathematics': {'accuracy': 0.6, 'total': 10, 'correct': 6, 'accuracy_percent': 60.0}, 'clinical_knowledge': {'accuracy': 0.8888888888888888, 'total': 9, 'correct': 8, 'accuracy_percent': 88.89}, 'security_studies': {'accuracy': 0.7777777777777778, 'total': 9, 'correct': 7, 'accuracy_percent': 77.78}, 'high_school_microeconomics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_world_history': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'human_aging': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_statistics': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'high_school_us_history': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_chemistry': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'sociology': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_geography': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'world_religions': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'virology': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'high_school_european_history': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'logical_fallacies': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'astronomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'high_school_physics': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'electrical_engineering': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'college_biology': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'anatomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'human_sexuality': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'econometrics': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'machine_learning': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'public_relations': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'jurisprudence': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'management': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'business_ethics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_chemistry': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_computer_science': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_mathematics': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.4879518072289157, 'total': 166, 'correct': 81, 'accuracy_percent': 48.8}, 'Other': {'accuracy': 0.6517857142857143, 'total': 112, 'correct': 73, 'accuracy_percent': 65.18}, 'Social_Sciences': {'accuracy': 0.6636363636363637, 'total': 110, 'correct': 73, 'accuracy_percent': 66.36}, 'STEM': {'accuracy': 0.4642857142857143, 'total': 112, 'correct': 52, 'accuracy_percent': 46.43}}",0.0
"gemma-2-2b_duplication_[(12,1),(13,1)]",H,middle,in_place,small,True,0.5,500,250,50.0,"{'professional_law': {'accuracy': 0.2909090909090909, 'total': 55, 'correct': 16, 'accuracy_percent': 29.09}, 'moral_scenarios': {'accuracy': 0.25, 'total': 32, 'correct': 8, 'accuracy_percent': 25.0}, 'miscellaneous': {'accuracy': 0.6785714285714286, 'total': 28, 'correct': 19, 'accuracy_percent': 67.86}, 'professional_psychology': {'accuracy': 0.36363636363636365, 'total': 22, 'correct': 8, 'accuracy_percent': 36.36}, 'high_school_psychology': {'accuracy': 0.6842105263157895, 'total': 19, 'correct': 13, 'accuracy_percent': 68.42}, 'high_school_macroeconomics': {'accuracy': 0.5, 'total': 14, 'correct': 7, 'accuracy_percent': 50.0}, 'elementary_mathematics': {'accuracy': 0.38461538461538464, 'total': 13, 'correct': 5, 'accuracy_percent': 38.46}, 'moral_disputes': {'accuracy': 0.4166666666666667, 'total': 12, 'correct': 5, 'accuracy_percent': 41.67}, 'prehistory': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'philosophy': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'high_school_biology': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'nutrition': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'professional_accounting': {'accuracy': 0.6, 'total': 10, 'correct': 6, 'accuracy_percent': 60.0}, 'professional_medicine': {'accuracy': 0.8, 'total': 10, 'correct': 8, 'accuracy_percent': 80.0}, 'high_school_mathematics': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'clinical_knowledge': {'accuracy': 0.7777777777777778, 'total': 9, 'correct': 7, 'accuracy_percent': 77.78}, 'security_studies': {'accuracy': 0.6666666666666666, 'total': 9, 'correct': 6, 'accuracy_percent': 66.67}, 'high_school_microeconomics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_world_history': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'conceptual_physics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'marketing': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_us_history': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'high_school_chemistry': {'accuracy': 0.14285714285714285, 'total': 7, 'correct': 1, 'accuracy_percent': 14.29}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_government_and_politics': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'college_medicine': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'world_religions': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'virology': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'high_school_european_history': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'logical_fallacies': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'astronomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'high_school_physics': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'electrical_engineering': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'college_biology': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'anatomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'human_sexuality': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'econometrics': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'machine_learning': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'public_relations': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'jurisprudence': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'management': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'business_ethics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_chemistry': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_computer_science': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.40963855421686746, 'total': 166, 'correct': 68, 'accuracy_percent': 40.96}, 'Other': {'accuracy': 0.6339285714285714, 'total': 112, 'correct': 71, 'accuracy_percent': 63.39}, 'Social_Sciences': {'accuracy': 0.5727272727272728, 'total': 110, 'correct': 63, 'accuracy_percent': 57.27}, 'STEM': {'accuracy': 0.42857142857142855, 'total': 112, 'correct': 48, 'accuracy_percent': 42.86}}",-10.39
"gemma-2-2b_duplication_[(22,4)]",E,last,blocks,medium,False,0.482,500,241,48.2,"{'professional_law': {'accuracy': 0.34545454545454546, 'total': 55, 'correct': 19, 'accuracy_percent': 34.55}, 'moral_scenarios': {'accuracy': 0.25, 'total': 32, 'correct': 8, 'accuracy_percent': 25.0}, 'miscellaneous': {'accuracy': 0.6428571428571429, 'total': 28, 'correct': 18, 'accuracy_percent': 64.29}, 'professional_psychology': {'accuracy': 0.45454545454545453, 'total': 22, 'correct': 10, 'accuracy_percent': 45.45}, 'high_school_psychology': {'accuracy': 0.7894736842105263, 'total': 19, 'correct': 15, 'accuracy_percent': 78.95}, 'high_school_macroeconomics': {'accuracy': 0.42857142857142855, 'total': 14, 'correct': 6, 'accuracy_percent': 42.86}, 'elementary_mathematics': {'accuracy': 0.15384615384615385, 'total': 13, 'correct': 2, 'accuracy_percent': 15.38}, 'moral_disputes': {'accuracy': 0.4166666666666667, 'total': 12, 'correct': 5, 'accuracy_percent': 41.67}, 'prehistory': {'accuracy': 0.36363636363636365, 'total': 11, 'correct': 4, 'accuracy_percent': 36.36}, 'philosophy': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'high_school_biology': {'accuracy': 0.36363636363636365, 'total': 11, 'correct': 4, 'accuracy_percent': 36.36}, 'nutrition': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'professional_accounting': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'professional_medicine': {'accuracy': 0.3, 'total': 10, 'correct': 3, 'accuracy_percent': 30.0}, 'high_school_mathematics': {'accuracy': 0.6, 'total': 10, 'correct': 6, 'accuracy_percent': 60.0}, 'clinical_knowledge': {'accuracy': 0.7777777777777778, 'total': 9, 'correct': 7, 'accuracy_percent': 77.78}, 'security_studies': {'accuracy': 0.3333333333333333, 'total': 9, 'correct': 3, 'accuracy_percent': 33.33}, 'high_school_microeconomics': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'high_school_world_history': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_us_history': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_chemistry': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'sociology': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_geography': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'world_religions': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'virology': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'high_school_european_history': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'high_school_physics': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'electrical_engineering': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'college_biology': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'anatomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'human_sexuality': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'public_relations': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'jurisprudence': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'management': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'business_ethics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_chemistry': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.39759036144578314, 'total': 166, 'correct': 66, 'accuracy_percent': 39.76}, 'Other': {'accuracy': 0.5625, 'total': 112, 'correct': 63, 'accuracy_percent': 56.25}, 'Social_Sciences': {'accuracy': 0.6181818181818182, 'total': 110, 'correct': 68, 'accuracy_percent': 61.82}, 'STEM': {'accuracy': 0.39285714285714285, 'total': 112, 'correct': 44, 'accuracy_percent': 39.29}}",-13.62
"gemma-2-2b_duplication_[(10,5)]",C,middle,blocks,big,False,0.478,500,239,47.8,"{'professional_law': {'accuracy': 0.36363636363636365, 'total': 55, 'correct': 20, 'accuracy_percent': 36.36}, 'moral_scenarios': {'accuracy': 0.28125, 'total': 32, 'correct': 9, 'accuracy_percent': 28.12}, 'miscellaneous': {'accuracy': 0.5714285714285714, 'total': 28, 'correct': 16, 'accuracy_percent': 57.14}, 'professional_psychology': {'accuracy': 0.4090909090909091, 'total': 22, 'correct': 9, 'accuracy_percent': 40.91}, 'high_school_psychology': {'accuracy': 0.631578947368421, 'total': 19, 'correct': 12, 'accuracy_percent': 63.16}, 'high_school_macroeconomics': {'accuracy': 0.35714285714285715, 'total': 14, 'correct': 5, 'accuracy_percent': 35.71}, 'elementary_mathematics': {'accuracy': 0.3076923076923077, 'total': 13, 'correct': 4, 'accuracy_percent': 30.77}, 'moral_disputes': {'accuracy': 0.5, 'total': 12, 'correct': 6, 'accuracy_percent': 50.0}, 'prehistory': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'philosophy': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'high_school_biology': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'nutrition': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'professional_accounting': {'accuracy': 0.6, 'total': 10, 'correct': 6, 'accuracy_percent': 60.0}, 'professional_medicine': {'accuracy': 0.2, 'total': 10, 'correct': 2, 'accuracy_percent': 20.0}, 'high_school_mathematics': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'clinical_knowledge': {'accuracy': 0.7777777777777778, 'total': 9, 'correct': 7, 'accuracy_percent': 77.78}, 'security_studies': {'accuracy': 0.5555555555555556, 'total': 9, 'correct': 5, 'accuracy_percent': 55.56}, 'high_school_microeconomics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'high_school_world_history': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_us_history': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_chemistry': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'sociology': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_geography': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'world_religions': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'virology': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'astronomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'high_school_physics': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'electrical_engineering': {'accuracy': 0.0, 'total': 5, 'correct': 0, 'accuracy_percent': 0.0}, 'college_biology': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'anatomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'human_sexuality': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'public_relations': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'jurisprudence': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'management': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'business_ethics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'high_school_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_chemistry': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.45180722891566266, 'total': 166, 'correct': 75, 'accuracy_percent': 45.18}, 'Other': {'accuracy': 0.5357142857142857, 'total': 112, 'correct': 60, 'accuracy_percent': 53.57}, 'Social_Sciences': {'accuracy': 0.5636363636363636, 'total': 110, 'correct': 62, 'accuracy_percent': 56.36}, 'STEM': {'accuracy': 0.375, 'total': 112, 'correct': 42, 'accuracy_percent': 37.5}}",-14.34
"gemma-2-2b_duplication_[(11,3)]",A,middle,blocks,medium,False,0.466,500,233,46.6,"{'professional_law': {'accuracy': 0.3090909090909091, 'total': 55, 'correct': 17, 'accuracy_percent': 30.91}, 'moral_scenarios': {'accuracy': 0.34375, 'total': 32, 'correct': 11, 'accuracy_percent': 34.38}, 'miscellaneous': {'accuracy': 0.5, 'total': 28, 'correct': 14, 'accuracy_percent': 50.0}, 'professional_psychology': {'accuracy': 0.4090909090909091, 'total': 22, 'correct': 9, 'accuracy_percent': 40.91}, 'high_school_psychology': {'accuracy': 0.7894736842105263, 'total': 19, 'correct': 15, 'accuracy_percent': 78.95}, 'high_school_macroeconomics': {'accuracy': 0.35714285714285715, 'total': 14, 'correct': 5, 'accuracy_percent': 35.71}, 'elementary_mathematics': {'accuracy': 0.38461538461538464, 'total': 13, 'correct': 5, 'accuracy_percent': 38.46}, 'moral_disputes': {'accuracy': 0.25, 'total': 12, 'correct': 3, 'accuracy_percent': 25.0}, 'prehistory': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'philosophy': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'high_school_biology': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'nutrition': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'professional_accounting': {'accuracy': 0.5, 'total': 10, 'correct': 5, 'accuracy_percent': 50.0}, 'professional_medicine': {'accuracy': 0.5, 'total': 10, 'correct': 5, 'accuracy_percent': 50.0}, 'high_school_mathematics': {'accuracy': 0.3, 'total': 10, 'correct': 3, 'accuracy_percent': 30.0}, 'clinical_knowledge': {'accuracy': 0.7777777777777778, 'total': 9, 'correct': 7, 'accuracy_percent': 77.78}, 'security_studies': {'accuracy': 0.4444444444444444, 'total': 9, 'correct': 4, 'accuracy_percent': 44.44}, 'high_school_microeconomics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_world_history': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.25, 'total': 8, 'correct': 2, 'accuracy_percent': 25.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'high_school_us_history': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_chemistry': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'high_school_government_and_politics': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'college_medicine': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'world_religions': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'virology': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'high_school_european_history': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'logical_fallacies': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'astronomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'high_school_physics': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'electrical_engineering': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'college_biology': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'anatomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'human_sexuality': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'formal_logic': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'international_law': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'econometrics': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'machine_learning': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'public_relations': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'jurisprudence': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'management': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_physics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'us_foreign_policy': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'global_facts': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'business_ethics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_chemistry': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.41566265060240964, 'total': 166, 'correct': 69, 'accuracy_percent': 41.57}, 'Other': {'accuracy': 0.6071428571428571, 'total': 112, 'correct': 68, 'accuracy_percent': 60.71}, 'Social_Sciences': {'accuracy': 0.5181818181818182, 'total': 110, 'correct': 57, 'accuracy_percent': 51.82}, 'STEM': {'accuracy': 0.3482142857142857, 'total': 112, 'correct': 39, 'accuracy_percent': 34.82}}",-16.49
"gemma-2-2b_duplication_[(0,1),(25,1)]",G,"first, last",in_place,small,False,0.422,500,211,42.2,"{'professional_law': {'accuracy': 0.38181818181818183, 'total': 55, 'correct': 21, 'accuracy_percent': 38.18}, 'moral_scenarios': {'accuracy': 0.25, 'total': 32, 'correct': 8, 'accuracy_percent': 25.0}, 'miscellaneous': {'accuracy': 0.5357142857142857, 'total': 28, 'correct': 15, 'accuracy_percent': 53.57}, 'professional_psychology': {'accuracy': 0.36363636363636365, 'total': 22, 'correct': 8, 'accuracy_percent': 36.36}, 'high_school_psychology': {'accuracy': 0.6842105263157895, 'total': 19, 'correct': 13, 'accuracy_percent': 68.42}, 'high_school_macroeconomics': {'accuracy': 0.42857142857142855, 'total': 14, 'correct': 6, 'accuracy_percent': 42.86}, 'elementary_mathematics': {'accuracy': 0.23076923076923078, 'total': 13, 'correct': 3, 'accuracy_percent': 23.08}, 'moral_disputes': {'accuracy': 0.5, 'total': 12, 'correct': 6, 'accuracy_percent': 50.0}, 'prehistory': {'accuracy': 0.36363636363636365, 'total': 11, 'correct': 4, 'accuracy_percent': 36.36}, 'philosophy': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'high_school_biology': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'nutrition': {'accuracy': 0.36363636363636365, 'total': 11, 'correct': 4, 'accuracy_percent': 36.36}, 'professional_accounting': {'accuracy': 0.5, 'total': 10, 'correct': 5, 'accuracy_percent': 50.0}, 'professional_medicine': {'accuracy': 0.0, 'total': 10, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_mathematics': {'accuracy': 0.2, 'total': 10, 'correct': 2, 'accuracy_percent': 20.0}, 'clinical_knowledge': {'accuracy': 0.5555555555555556, 'total': 9, 'correct': 5, 'accuracy_percent': 55.56}, 'security_studies': {'accuracy': 0.2222222222222222, 'total': 9, 'correct': 2, 'accuracy_percent': 22.22}, 'high_school_microeconomics': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_world_history': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_us_history': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'high_school_chemistry': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'sociology': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'high_school_geography': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'world_religions': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'virology': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'astronomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'high_school_physics': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'electrical_engineering': {'accuracy': 0.0, 'total': 5, 'correct': 0, 'accuracy_percent': 0.0}, 'college_biology': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'anatomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'human_sexuality': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'econometrics': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'machine_learning': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'public_relations': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'jurisprudence': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'management': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_physics': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'us_foreign_policy': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'global_facts': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'business_ethics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'high_school_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_chemistry': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.3795180722891566, 'total': 166, 'correct': 63, 'accuracy_percent': 37.95}, 'Other': {'accuracy': 0.45535714285714285, 'total': 112, 'correct': 51, 'accuracy_percent': 45.54}, 'Social_Sciences': {'accuracy': 0.5363636363636364, 'total': 110, 'correct': 59, 'accuracy_percent': 53.64}, 'STEM': {'accuracy': 0.3392857142857143, 'total': 112, 'correct': 38, 'accuracy_percent': 33.93}}",-24.37
"gemma-2-2b_duplication_[(0,1),(12,1),(25,1)]",I,"first, middle, last",in_place,small,False,0.412,500,206,41.2,"{'professional_law': {'accuracy': 0.38181818181818183, 'total': 55, 'correct': 21, 'accuracy_percent': 38.18}, 'moral_scenarios': {'accuracy': 0.25, 'total': 32, 'correct': 8, 'accuracy_percent': 25.0}, 'miscellaneous': {'accuracy': 0.5, 'total': 28, 'correct': 14, 'accuracy_percent': 50.0}, 'professional_psychology': {'accuracy': 0.36363636363636365, 'total': 22, 'correct': 8, 'accuracy_percent': 36.36}, 'high_school_psychology': {'accuracy': 0.631578947368421, 'total': 19, 'correct': 12, 'accuracy_percent': 63.16}, 'high_school_macroeconomics': {'accuracy': 0.5, 'total': 14, 'correct': 7, 'accuracy_percent': 50.0}, 'elementary_mathematics': {'accuracy': 0.15384615384615385, 'total': 13, 'correct': 2, 'accuracy_percent': 15.38}, 'moral_disputes': {'accuracy': 0.3333333333333333, 'total': 12, 'correct': 4, 'accuracy_percent': 33.33}, 'prehistory': {'accuracy': 0.2727272727272727, 'total': 11, 'correct': 3, 'accuracy_percent': 27.27}, 'philosophy': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'high_school_biology': {'accuracy': 0.8181818181818182, 'total': 11, 'correct': 9, 'accuracy_percent': 81.82}, 'nutrition': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'professional_accounting': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'professional_medicine': {'accuracy': 0.1, 'total': 10, 'correct': 1, 'accuracy_percent': 10.0}, 'high_school_mathematics': {'accuracy': 0.3, 'total': 10, 'correct': 3, 'accuracy_percent': 30.0}, 'clinical_knowledge': {'accuracy': 0.5555555555555556, 'total': 9, 'correct': 5, 'accuracy_percent': 55.56}, 'security_studies': {'accuracy': 0.2222222222222222, 'total': 9, 'correct': 2, 'accuracy_percent': 22.22}, 'high_school_microeconomics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_world_history': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'conceptual_physics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'marketing': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_us_history': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'high_school_chemistry': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'sociology': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'high_school_geography': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_government_and_politics': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'college_medicine': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'world_religions': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'virology': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'astronomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'high_school_physics': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'electrical_engineering': {'accuracy': 0.0, 'total': 5, 'correct': 0, 'accuracy_percent': 0.0}, 'college_biology': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'anatomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'human_sexuality': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'public_relations': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'jurisprudence': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'management': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_physics': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'us_foreign_policy': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'global_facts': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'business_ethics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'high_school_computer_science': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_chemistry': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.35542168674698793, 'total': 166, 'correct': 59, 'accuracy_percent': 35.54}, 'Other': {'accuracy': 0.42857142857142855, 'total': 112, 'correct': 48, 'accuracy_percent': 42.86}, 'Social_Sciences': {'accuracy': 0.5181818181818182, 'total': 110, 'correct': 57, 'accuracy_percent': 51.82}, 'STEM': {'accuracy': 0.375, 'total': 112, 'correct': 42, 'accuracy_percent': 37.5}}",-26.16
"gemma-2-2b_duplication_[(13,2)]",B,middle,in_place,medium,False,0.384,500,192,38.4,"{'professional_law': {'accuracy': 0.2909090909090909, 'total': 55, 'correct': 16, 'accuracy_percent': 29.09}, 'moral_scenarios': {'accuracy': 0.28125, 'total': 32, 'correct': 9, 'accuracy_percent': 28.12}, 'miscellaneous': {'accuracy': 0.35714285714285715, 'total': 28, 'correct': 10, 'accuracy_percent': 35.71}, 'professional_psychology': {'accuracy': 0.4090909090909091, 'total': 22, 'correct': 9, 'accuracy_percent': 40.91}, 'high_school_psychology': {'accuracy': 0.47368421052631576, 'total': 19, 'correct': 9, 'accuracy_percent': 47.37}, 'high_school_macroeconomics': {'accuracy': 0.14285714285714285, 'total': 14, 'correct': 2, 'accuracy_percent': 14.29}, 'elementary_mathematics': {'accuracy': 0.5384615384615384, 'total': 13, 'correct': 7, 'accuracy_percent': 53.85}, 'moral_disputes': {'accuracy': 0.4166666666666667, 'total': 12, 'correct': 5, 'accuracy_percent': 41.67}, 'prehistory': {'accuracy': 0.2727272727272727, 'total': 11, 'correct': 3, 'accuracy_percent': 27.27}, 'philosophy': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'high_school_biology': {'accuracy': 0.36363636363636365, 'total': 11, 'correct': 4, 'accuracy_percent': 36.36}, 'nutrition': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'professional_accounting': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'professional_medicine': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'high_school_mathematics': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'clinical_knowledge': {'accuracy': 0.5555555555555556, 'total': 9, 'correct': 5, 'accuracy_percent': 55.56}, 'security_studies': {'accuracy': 0.5555555555555556, 'total': 9, 'correct': 5, 'accuracy_percent': 55.56}, 'high_school_microeconomics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'high_school_world_history': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'conceptual_physics': {'accuracy': 0.25, 'total': 8, 'correct': 2, 'accuracy_percent': 25.0}, 'marketing': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.125, 'total': 8, 'correct': 1, 'accuracy_percent': 12.5}, 'high_school_us_history': {'accuracy': 0.14285714285714285, 'total': 7, 'correct': 1, 'accuracy_percent': 14.29}, 'high_school_chemistry': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'sociology': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'high_school_geography': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'high_school_government_and_politics': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'college_medicine': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'world_religions': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'virology': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'high_school_european_history': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'logical_fallacies': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'astronomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'high_school_physics': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'electrical_engineering': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'college_biology': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'anatomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'human_sexuality': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'formal_logic': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'international_law': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'econometrics': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'machine_learning': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'public_relations': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'jurisprudence': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'management': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_physics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'us_foreign_policy': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'global_facts': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'business_ethics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'high_school_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_chemistry': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_computer_science': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}}","{'Humanities': {'accuracy': 0.3373493975903614, 'total': 166, 'correct': 56, 'accuracy_percent': 33.73}, 'Other': {'accuracy': 0.4642857142857143, 'total': 112, 'correct': 52, 'accuracy_percent': 46.43}, 'Social_Sciences': {'accuracy': 0.4090909090909091, 'total': 110, 'correct': 45, 'accuracy_percent': 40.91}, 'STEM': {'accuracy': 0.3482142857142857, 'total': 112, 'correct': 39, 'accuracy_percent': 34.82}}",-31.18
"gemma-2-2b_duplication_[(0,4)]",D,first,blocks,big,False,0.336,500,168,33.6,"{'professional_law': {'accuracy': 0.2, 'total': 55, 'correct': 11, 'accuracy_percent': 20.0}, 'moral_scenarios': {'accuracy': 0.25, 'total': 32, 'correct': 8, 'accuracy_percent': 25.0}, 'miscellaneous': {'accuracy': 0.35714285714285715, 'total': 28, 'correct': 10, 'accuracy_percent': 35.71}, 'professional_psychology': {'accuracy': 0.4090909090909091, 'total': 22, 'correct': 9, 'accuracy_percent': 40.91}, 'high_school_psychology': {'accuracy': 0.21052631578947367, 'total': 19, 'correct': 4, 'accuracy_percent': 21.05}, 'high_school_macroeconomics': {'accuracy': 0.2857142857142857, 'total': 14, 'correct': 4, 'accuracy_percent': 28.57}, 'elementary_mathematics': {'accuracy': 0.07692307692307693, 'total': 13, 'correct': 1, 'accuracy_percent': 7.69}, 'moral_disputes': {'accuracy': 0.4166666666666667, 'total': 12, 'correct': 5, 'accuracy_percent': 41.67}, 'prehistory': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'philosophy': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'high_school_biology': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'nutrition': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'professional_accounting': {'accuracy': 0.1, 'total': 10, 'correct': 1, 'accuracy_percent': 10.0}, 'professional_medicine': {'accuracy': 0.1, 'total': 10, 'correct': 1, 'accuracy_percent': 10.0}, 'high_school_mathematics': {'accuracy': 0.0, 'total': 10, 'correct': 0, 'accuracy_percent': 0.0}, 'clinical_knowledge': {'accuracy': 0.4444444444444444, 'total': 9, 'correct': 4, 'accuracy_percent': 44.44}, 'security_studies': {'accuracy': 0.3333333333333333, 'total': 9, 'correct': 3, 'accuracy_percent': 33.33}, 'high_school_microeconomics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_world_history': {'accuracy': 0.25, 'total': 8, 'correct': 2, 'accuracy_percent': 25.0}, 'conceptual_physics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'marketing': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'human_aging': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_statistics': {'accuracy': 0.125, 'total': 8, 'correct': 1, 'accuracy_percent': 12.5}, 'high_school_us_history': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'high_school_chemistry': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'sociology': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'high_school_geography': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'high_school_government_and_politics': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'college_medicine': {'accuracy': 0.0, 'total': 6, 'correct': 0, 'accuracy_percent': 0.0}, 'world_religions': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'virology': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'astronomy': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'high_school_physics': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'electrical_engineering': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'college_biology': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'anatomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'human_sexuality': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'formal_logic': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'international_law': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'public_relations': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'jurisprudence': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'management': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_physics': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'business_ethics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'high_school_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_chemistry': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}}","{'Humanities': {'accuracy': 0.3253012048192771, 'total': 166, 'correct': 54, 'accuracy_percent': 32.53}, 'Other': {'accuracy': 0.3392857142857143, 'total': 112, 'correct': 38, 'accuracy_percent': 33.93}, 'Social_Sciences': {'accuracy': 0.4, 'total': 110, 'correct': 44, 'accuracy_percent': 40.0}, 'STEM': {'accuracy': 0.2857142857142857, 'total': 112, 'correct': 32, 'accuracy_percent': 28.57}}",-39.78
"gemma-2-2b_duplication_[(0,3),(22,3)]",F,"first, last",blocks,medium,False,0.304,500,152,30.4,"{'professional_law': {'accuracy': 0.2727272727272727, 'total': 55, 'correct': 15, 'accuracy_percent': 27.27}, 'moral_scenarios': {'accuracy': 0.25, 'total': 32, 'correct': 8, 'accuracy_percent': 25.0}, 'miscellaneous': {'accuracy': 0.4642857142857143, 'total': 28, 'correct': 13, 'accuracy_percent': 46.43}, 'professional_psychology': {'accuracy': 0.5, 'total': 22, 'correct': 11, 'accuracy_percent': 50.0}, 'high_school_psychology': {'accuracy': 0.3157894736842105, 'total': 19, 'correct': 6, 'accuracy_percent': 31.58}, 'high_school_macroeconomics': {'accuracy': 0.21428571428571427, 'total': 14, 'correct': 3, 'accuracy_percent': 21.43}, 'elementary_mathematics': {'accuracy': 0.0, 'total': 13, 'correct': 0, 'accuracy_percent': 0.0}, 'moral_disputes': {'accuracy': 0.4166666666666667, 'total': 12, 'correct': 5, 'accuracy_percent': 41.67}, 'prehistory': {'accuracy': 0.2727272727272727, 'total': 11, 'correct': 3, 'accuracy_percent': 27.27}, 'philosophy': {'accuracy': 0.36363636363636365, 'total': 11, 'correct': 4, 'accuracy_percent': 36.36}, 'high_school_biology': {'accuracy': 0.36363636363636365, 'total': 11, 'correct': 4, 'accuracy_percent': 36.36}, 'nutrition': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'professional_accounting': {'accuracy': 0.3, 'total': 10, 'correct': 3, 'accuracy_percent': 30.0}, 'professional_medicine': {'accuracy': 0.0, 'total': 10, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_mathematics': {'accuracy': 0.3, 'total': 10, 'correct': 3, 'accuracy_percent': 30.0}, 'clinical_knowledge': {'accuracy': 0.3333333333333333, 'total': 9, 'correct': 3, 'accuracy_percent': 33.33}, 'security_studies': {'accuracy': 0.2222222222222222, 'total': 9, 'correct': 2, 'accuracy_percent': 22.22}, 'high_school_microeconomics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_world_history': {'accuracy': 0.125, 'total': 8, 'correct': 1, 'accuracy_percent': 12.5}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.25, 'total': 8, 'correct': 2, 'accuracy_percent': 25.0}, 'human_aging': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'high_school_statistics': {'accuracy': 0.25, 'total': 8, 'correct': 2, 'accuracy_percent': 25.0}, 'high_school_us_history': {'accuracy': 0.14285714285714285, 'total': 7, 'correct': 1, 'accuracy_percent': 14.29}, 'high_school_chemistry': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'sociology': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'high_school_geography': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'high_school_government_and_politics': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'college_medicine': {'accuracy': 0.0, 'total': 6, 'correct': 0, 'accuracy_percent': 0.0}, 'world_religions': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'virology': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'high_school_european_history': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'logical_fallacies': {'accuracy': 0.16666666666666666, 'total': 6, 'correct': 1, 'accuracy_percent': 16.67}, 'astronomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'high_school_physics': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'electrical_engineering': {'accuracy': 0.0, 'total': 5, 'correct': 0, 'accuracy_percent': 0.0}, 'college_biology': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'anatomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'human_sexuality': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'econometrics': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'machine_learning': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'public_relations': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'jurisprudence': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'management': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_physics': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'us_foreign_policy': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'global_facts': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'business_ethics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'high_school_computer_science': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_chemistry': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}}","{'Humanities': {'accuracy': 0.2710843373493976, 'total': 166, 'correct': 45, 'accuracy_percent': 27.11}, 'Other': {'accuracy': 0.32142857142857145, 'total': 112, 'correct': 36, 'accuracy_percent': 32.14}, 'Social_Sciences': {'accuracy': 0.4090909090909091, 'total': 110, 'correct': 45, 'accuracy_percent': 40.91}, 'STEM': {'accuracy': 0.23214285714285715, 'total': 112, 'correct': 26, 'accuracy_percent': 23.21}}",-45.52
