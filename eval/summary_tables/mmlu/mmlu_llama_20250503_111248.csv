configuration,model,accuracy,total,correct,accuracy_percent,per_subject,per_super_category,% improvement
original,Meta-Llama-3-8B,0.656,500,328,65.6,"{'professional_law': {'accuracy': 0.4, 'total': 55, 'correct': 22, 'accuracy_percent': 40.0}, 'moral_scenarios': {'accuracy': 0.46875, 'total': 32, 'correct': 15, 'accuracy_percent': 46.88}, 'miscellaneous': {'accuracy': 0.7857142857142857, 'total': 28, 'correct': 22, 'accuracy_percent': 78.57}, 'professional_psychology': {'accuracy': 0.7727272727272727, 'total': 22, 'correct': 17, 'accuracy_percent': 77.27}, 'high_school_psychology': {'accuracy': 0.9473684210526315, 'total': 19, 'correct': 18, 'accuracy_percent': 94.74}, 'high_school_macroeconomics': {'accuracy': 0.5, 'total': 14, 'correct': 7, 'accuracy_percent': 50.0}, 'elementary_mathematics': {'accuracy': 0.3076923076923077, 'total': 13, 'correct': 4, 'accuracy_percent': 30.77}, 'moral_disputes': {'accuracy': 0.8333333333333334, 'total': 12, 'correct': 10, 'accuracy_percent': 83.33}, 'prehistory': {'accuracy': 0.8181818181818182, 'total': 11, 'correct': 9, 'accuracy_percent': 81.82}, 'philosophy': {'accuracy': 0.8181818181818182, 'total': 11, 'correct': 9, 'accuracy_percent': 81.82}, 'high_school_biology': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'nutrition': {'accuracy': 0.9090909090909091, 'total': 11, 'correct': 10, 'accuracy_percent': 90.91}, 'professional_accounting': {'accuracy': 0.7, 'total': 10, 'correct': 7, 'accuracy_percent': 70.0}, 'professional_medicine': {'accuracy': 0.8, 'total': 10, 'correct': 8, 'accuracy_percent': 80.0}, 'high_school_mathematics': {'accuracy': 0.6, 'total': 10, 'correct': 6, 'accuracy_percent': 60.0}, 'clinical_knowledge': {'accuracy': 1.0, 'total': 9, 'correct': 9, 'accuracy_percent': 100.0}, 'security_studies': {'accuracy': 0.6666666666666666, 'total': 9, 'correct': 6, 'accuracy_percent': 66.67}, 'high_school_microeconomics': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'high_school_world_history': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'conceptual_physics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'marketing': {'accuracy': 1.0, 'total': 8, 'correct': 8, 'accuracy_percent': 100.0}, 'human_aging': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_statistics': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'world_religions': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'virology': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'high_school_physics': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'electrical_engineering': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'college_biology': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'anatomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'human_sexuality': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'formal_logic': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'public_relations': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'jurisprudence': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'management': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'business_ethics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_chemistry': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_mathematics': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}, 'computer_security': {'accuracy': 1.0, 'total': 3, 'correct': 3, 'accuracy_percent': 100.0}}","{'Humanities': {'accuracy': 0.5481927710843374, 'total': 166, 'correct': 91, 'accuracy_percent': 54.82}, 'Other': {'accuracy': 0.7857142857142857, 'total': 112, 'correct': 88, 'accuracy_percent': 78.57}, 'Social_Sciences': {'accuracy': 0.7818181818181819, 'total': 110, 'correct': 86, 'accuracy_percent': 78.18}, 'STEM': {'accuracy': 0.5625, 'total': 112, 'correct': 63, 'accuracy_percent': 56.25}}",0.0
H,"Meta-Llama-3-8B_duplication_[(15,1),(16,1)]",0.652,500,326,65.2,"{'professional_law': {'accuracy': 0.4, 'total': 55, 'correct': 22, 'accuracy_percent': 40.0}, 'moral_scenarios': {'accuracy': 0.5, 'total': 32, 'correct': 16, 'accuracy_percent': 50.0}, 'miscellaneous': {'accuracy': 0.75, 'total': 28, 'correct': 21, 'accuracy_percent': 75.0}, 'professional_psychology': {'accuracy': 0.7272727272727273, 'total': 22, 'correct': 16, 'accuracy_percent': 72.73}, 'high_school_psychology': {'accuracy': 0.9473684210526315, 'total': 19, 'correct': 18, 'accuracy_percent': 94.74}, 'high_school_macroeconomics': {'accuracy': 0.42857142857142855, 'total': 14, 'correct': 6, 'accuracy_percent': 42.86}, 'elementary_mathematics': {'accuracy': 0.38461538461538464, 'total': 13, 'correct': 5, 'accuracy_percent': 38.46}, 'moral_disputes': {'accuracy': 0.8333333333333334, 'total': 12, 'correct': 10, 'accuracy_percent': 83.33}, 'prehistory': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'philosophy': {'accuracy': 0.8181818181818182, 'total': 11, 'correct': 9, 'accuracy_percent': 81.82}, 'high_school_biology': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'nutrition': {'accuracy': 0.9090909090909091, 'total': 11, 'correct': 10, 'accuracy_percent': 90.91}, 'professional_accounting': {'accuracy': 0.8, 'total': 10, 'correct': 8, 'accuracy_percent': 80.0}, 'professional_medicine': {'accuracy': 0.9, 'total': 10, 'correct': 9, 'accuracy_percent': 90.0}, 'high_school_mathematics': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'clinical_knowledge': {'accuracy': 1.0, 'total': 9, 'correct': 9, 'accuracy_percent': 100.0}, 'security_studies': {'accuracy': 0.6666666666666666, 'total': 9, 'correct': 6, 'accuracy_percent': 66.67}, 'high_school_microeconomics': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'high_school_world_history': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'conceptual_physics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'marketing': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'human_aging': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_statistics': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'world_religions': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'virology': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'high_school_physics': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'electrical_engineering': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'college_biology': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'anatomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'human_sexuality': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'formal_logic': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'machine_learning': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'public_relations': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'jurisprudence': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'management': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'business_ethics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_chemistry': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_mathematics': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}, 'computer_security': {'accuracy': 1.0, 'total': 3, 'correct': 3, 'accuracy_percent': 100.0}}","{'Humanities': {'accuracy': 0.536144578313253, 'total': 166, 'correct': 89, 'accuracy_percent': 53.61}, 'Other': {'accuracy': 0.7767857142857143, 'total': 112, 'correct': 87, 'accuracy_percent': 77.68}, 'Social_Sciences': {'accuracy': 0.7727272727272727, 'total': 110, 'correct': 85, 'accuracy_percent': 77.27}, 'STEM': {'accuracy': 0.5803571428571429, 'total': 112, 'correct': 65, 'accuracy_percent': 58.04}}",-0.61
C,"Meta-Llama-3-8B_duplication_[(13,5)]",0.644,500,322,64.4,"{'professional_law': {'accuracy': 0.4, 'total': 55, 'correct': 22, 'accuracy_percent': 40.0}, 'moral_scenarios': {'accuracy': 0.4375, 'total': 32, 'correct': 14, 'accuracy_percent': 43.75}, 'miscellaneous': {'accuracy': 0.75, 'total': 28, 'correct': 21, 'accuracy_percent': 75.0}, 'professional_psychology': {'accuracy': 0.7727272727272727, 'total': 22, 'correct': 17, 'accuracy_percent': 77.27}, 'high_school_psychology': {'accuracy': 0.9473684210526315, 'total': 19, 'correct': 18, 'accuracy_percent': 94.74}, 'high_school_macroeconomics': {'accuracy': 0.6428571428571429, 'total': 14, 'correct': 9, 'accuracy_percent': 64.29}, 'elementary_mathematics': {'accuracy': 0.23076923076923078, 'total': 13, 'correct': 3, 'accuracy_percent': 23.08}, 'moral_disputes': {'accuracy': 0.75, 'total': 12, 'correct': 9, 'accuracy_percent': 75.0}, 'prehistory': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'philosophy': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'high_school_biology': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'nutrition': {'accuracy': 0.9090909090909091, 'total': 11, 'correct': 10, 'accuracy_percent': 90.91}, 'professional_accounting': {'accuracy': 0.7, 'total': 10, 'correct': 7, 'accuracy_percent': 70.0}, 'professional_medicine': {'accuracy': 0.9, 'total': 10, 'correct': 9, 'accuracy_percent': 90.0}, 'high_school_mathematics': {'accuracy': 0.6, 'total': 10, 'correct': 6, 'accuracy_percent': 60.0}, 'clinical_knowledge': {'accuracy': 1.0, 'total': 9, 'correct': 9, 'accuracy_percent': 100.0}, 'security_studies': {'accuracy': 0.5555555555555556, 'total': 9, 'correct': 5, 'accuracy_percent': 55.56}, 'high_school_microeconomics': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'high_school_world_history': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'conceptual_physics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'marketing': {'accuracy': 1.0, 'total': 8, 'correct': 8, 'accuracy_percent': 100.0}, 'human_aging': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_statistics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'world_religions': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'virology': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 1.0, 'total': 6, 'correct': 6, 'accuracy_percent': 100.0}, 'astronomy': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'high_school_physics': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'electrical_engineering': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'college_biology': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'anatomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'human_sexuality': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'public_relations': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'jurisprudence': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'management': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'business_ethics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_chemistry': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_mathematics': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}, 'computer_security': {'accuracy': 1.0, 'total': 3, 'correct': 3, 'accuracy_percent': 100.0}}","{'Humanities': {'accuracy': 0.5240963855421686, 'total': 166, 'correct': 87, 'accuracy_percent': 52.41}, 'Other': {'accuracy': 0.7857142857142857, 'total': 112, 'correct': 88, 'accuracy_percent': 78.57}, 'Social_Sciences': {'accuracy': 0.8, 'total': 110, 'correct': 88, 'accuracy_percent': 80.0}, 'STEM': {'accuracy': 0.5267857142857143, 'total': 112, 'correct': 59, 'accuracy_percent': 52.68}}",-1.83
E,"Meta-Llama-3-8B_duplication_[(27,5)]",0.636,500,318,63.6,"{'professional_law': {'accuracy': 0.38181818181818183, 'total': 55, 'correct': 21, 'accuracy_percent': 38.18}, 'moral_scenarios': {'accuracy': 0.46875, 'total': 32, 'correct': 15, 'accuracy_percent': 46.88}, 'miscellaneous': {'accuracy': 0.75, 'total': 28, 'correct': 21, 'accuracy_percent': 75.0}, 'professional_psychology': {'accuracy': 0.7727272727272727, 'total': 22, 'correct': 17, 'accuracy_percent': 77.27}, 'high_school_psychology': {'accuracy': 0.9473684210526315, 'total': 19, 'correct': 18, 'accuracy_percent': 94.74}, 'high_school_macroeconomics': {'accuracy': 0.42857142857142855, 'total': 14, 'correct': 6, 'accuracy_percent': 42.86}, 'elementary_mathematics': {'accuracy': 0.3076923076923077, 'total': 13, 'correct': 4, 'accuracy_percent': 30.77}, 'moral_disputes': {'accuracy': 0.8333333333333334, 'total': 12, 'correct': 10, 'accuracy_percent': 83.33}, 'prehistory': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'philosophy': {'accuracy': 0.8181818181818182, 'total': 11, 'correct': 9, 'accuracy_percent': 81.82}, 'high_school_biology': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'nutrition': {'accuracy': 1.0, 'total': 11, 'correct': 11, 'accuracy_percent': 100.0}, 'professional_accounting': {'accuracy': 0.8, 'total': 10, 'correct': 8, 'accuracy_percent': 80.0}, 'professional_medicine': {'accuracy': 0.9, 'total': 10, 'correct': 9, 'accuracy_percent': 90.0}, 'high_school_mathematics': {'accuracy': 0.5, 'total': 10, 'correct': 5, 'accuracy_percent': 50.0}, 'clinical_knowledge': {'accuracy': 1.0, 'total': 9, 'correct': 9, 'accuracy_percent': 100.0}, 'security_studies': {'accuracy': 0.6666666666666666, 'total': 9, 'correct': 6, 'accuracy_percent': 66.67}, 'high_school_microeconomics': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_world_history': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'conceptual_physics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'marketing': {'accuracy': 1.0, 'total': 8, 'correct': 8, 'accuracy_percent': 100.0}, 'human_aging': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_statistics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'world_religions': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'virology': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'high_school_physics': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'electrical_engineering': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'college_biology': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'anatomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'human_sexuality': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'public_relations': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'jurisprudence': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'management': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'business_ethics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'high_school_computer_science': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_chemistry': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 1.0, 'total': 3, 'correct': 3, 'accuracy_percent': 100.0}}","{'Humanities': {'accuracy': 0.5301204819277109, 'total': 166, 'correct': 88, 'accuracy_percent': 53.01}, 'Other': {'accuracy': 0.75, 'total': 112, 'correct': 84, 'accuracy_percent': 75.0}, 'Social_Sciences': {'accuracy': 0.7727272727272727, 'total': 110, 'correct': 85, 'accuracy_percent': 77.27}, 'STEM': {'accuracy': 0.5446428571428571, 'total': 112, 'correct': 61, 'accuracy_percent': 54.46}}",-3.05
A,"Meta-Llama-3-8B_duplication_[(14,3)]",0.628,500,314,62.8,"{'professional_law': {'accuracy': 0.34545454545454546, 'total': 55, 'correct': 19, 'accuracy_percent': 34.55}, 'moral_scenarios': {'accuracy': 0.4375, 'total': 32, 'correct': 14, 'accuracy_percent': 43.75}, 'miscellaneous': {'accuracy': 0.7857142857142857, 'total': 28, 'correct': 22, 'accuracy_percent': 78.57}, 'professional_psychology': {'accuracy': 0.6818181818181818, 'total': 22, 'correct': 15, 'accuracy_percent': 68.18}, 'high_school_psychology': {'accuracy': 0.9473684210526315, 'total': 19, 'correct': 18, 'accuracy_percent': 94.74}, 'high_school_macroeconomics': {'accuracy': 0.5714285714285714, 'total': 14, 'correct': 8, 'accuracy_percent': 57.14}, 'elementary_mathematics': {'accuracy': 0.46153846153846156, 'total': 13, 'correct': 6, 'accuracy_percent': 46.15}, 'moral_disputes': {'accuracy': 0.8333333333333334, 'total': 12, 'correct': 10, 'accuracy_percent': 83.33}, 'prehistory': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'philosophy': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'high_school_biology': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'nutrition': {'accuracy': 0.9090909090909091, 'total': 11, 'correct': 10, 'accuracy_percent': 90.91}, 'professional_accounting': {'accuracy': 0.6, 'total': 10, 'correct': 6, 'accuracy_percent': 60.0}, 'professional_medicine': {'accuracy': 0.9, 'total': 10, 'correct': 9, 'accuracy_percent': 90.0}, 'high_school_mathematics': {'accuracy': 0.7, 'total': 10, 'correct': 7, 'accuracy_percent': 70.0}, 'clinical_knowledge': {'accuracy': 0.8888888888888888, 'total': 9, 'correct': 8, 'accuracy_percent': 88.89}, 'security_studies': {'accuracy': 0.5555555555555556, 'total': 9, 'correct': 5, 'accuracy_percent': 55.56}, 'high_school_microeconomics': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'high_school_world_history': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'conceptual_physics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'marketing': {'accuracy': 1.0, 'total': 8, 'correct': 8, 'accuracy_percent': 100.0}, 'human_aging': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_statistics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'world_religions': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'virology': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'high_school_physics': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'electrical_engineering': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'college_biology': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'anatomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'human_sexuality': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'machine_learning': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'public_relations': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'jurisprudence': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'management': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'business_ethics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_chemistry': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_computer_science': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 1.0, 'total': 3, 'correct': 3, 'accuracy_percent': 100.0}}","{'Humanities': {'accuracy': 0.5, 'total': 166, 'correct': 83, 'accuracy_percent': 50.0}, 'Other': {'accuracy': 0.7678571428571429, 'total': 112, 'correct': 86, 'accuracy_percent': 76.79}, 'Social_Sciences': {'accuracy': 0.7727272727272727, 'total': 110, 'correct': 85, 'accuracy_percent': 77.27}, 'STEM': {'accuracy': 0.5357142857142857, 'total': 112, 'correct': 60, 'accuracy_percent': 53.57}}",-4.27
B,"Meta-Llama-3-8B_duplication_[(16,2)]",0.626,500,313,62.6,"{'professional_law': {'accuracy': 0.38181818181818183, 'total': 55, 'correct': 21, 'accuracy_percent': 38.18}, 'moral_scenarios': {'accuracy': 0.46875, 'total': 32, 'correct': 15, 'accuracy_percent': 46.88}, 'miscellaneous': {'accuracy': 0.7857142857142857, 'total': 28, 'correct': 22, 'accuracy_percent': 78.57}, 'professional_psychology': {'accuracy': 0.7272727272727273, 'total': 22, 'correct': 16, 'accuracy_percent': 72.73}, 'high_school_psychology': {'accuracy': 0.8947368421052632, 'total': 19, 'correct': 17, 'accuracy_percent': 89.47}, 'high_school_macroeconomics': {'accuracy': 0.42857142857142855, 'total': 14, 'correct': 6, 'accuracy_percent': 42.86}, 'elementary_mathematics': {'accuracy': 0.38461538461538464, 'total': 13, 'correct': 5, 'accuracy_percent': 38.46}, 'moral_disputes': {'accuracy': 0.6666666666666666, 'total': 12, 'correct': 8, 'accuracy_percent': 66.67}, 'prehistory': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'philosophy': {'accuracy': 0.8181818181818182, 'total': 11, 'correct': 9, 'accuracy_percent': 81.82}, 'high_school_biology': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'nutrition': {'accuracy': 0.9090909090909091, 'total': 11, 'correct': 10, 'accuracy_percent': 90.91}, 'professional_accounting': {'accuracy': 0.6, 'total': 10, 'correct': 6, 'accuracy_percent': 60.0}, 'professional_medicine': {'accuracy': 1.0, 'total': 10, 'correct': 10, 'accuracy_percent': 100.0}, 'high_school_mathematics': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'clinical_knowledge': {'accuracy': 1.0, 'total': 9, 'correct': 9, 'accuracy_percent': 100.0}, 'security_studies': {'accuracy': 0.6666666666666666, 'total': 9, 'correct': 6, 'accuracy_percent': 66.67}, 'high_school_microeconomics': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_world_history': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'conceptual_physics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'marketing': {'accuracy': 1.0, 'total': 8, 'correct': 8, 'accuracy_percent': 100.0}, 'human_aging': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_statistics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_government_and_politics': {'accuracy': 1.0, 'total': 7, 'correct': 7, 'accuracy_percent': 100.0}, 'college_medicine': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'world_religions': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'virology': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'high_school_european_history': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'high_school_physics': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'electrical_engineering': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'college_biology': {'accuracy': 1.0, 'total': 5, 'correct': 5, 'accuracy_percent': 100.0}, 'anatomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'human_sexuality': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'public_relations': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'jurisprudence': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'management': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'business_ethics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'abstract_algebra': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_chemistry': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 1.0, 'total': 3, 'correct': 3, 'accuracy_percent': 100.0}}","{'Humanities': {'accuracy': 0.4939759036144578, 'total': 166, 'correct': 82, 'accuracy_percent': 49.4}, 'Other': {'accuracy': 0.8035714285714286, 'total': 112, 'correct': 90, 'accuracy_percent': 80.36}, 'Social_Sciences': {'accuracy': 0.7363636363636363, 'total': 110, 'correct': 81, 'accuracy_percent': 73.64}, 'STEM': {'accuracy': 0.5357142857142857, 'total': 112, 'correct': 60, 'accuracy_percent': 53.57}}",-4.57
G,"Meta-Llama-3-8B_duplication_[(0,1),(31,1)]",0.492,500,246,49.2,"{'professional_law': {'accuracy': 0.4, 'total': 55, 'correct': 22, 'accuracy_percent': 40.0}, 'moral_scenarios': {'accuracy': 0.25, 'total': 32, 'correct': 8, 'accuracy_percent': 25.0}, 'miscellaneous': {'accuracy': 0.5, 'total': 28, 'correct': 14, 'accuracy_percent': 50.0}, 'professional_psychology': {'accuracy': 0.5454545454545454, 'total': 22, 'correct': 12, 'accuracy_percent': 54.55}, 'high_school_psychology': {'accuracy': 0.7894736842105263, 'total': 19, 'correct': 15, 'accuracy_percent': 78.95}, 'high_school_macroeconomics': {'accuracy': 0.35714285714285715, 'total': 14, 'correct': 5, 'accuracy_percent': 35.71}, 'elementary_mathematics': {'accuracy': 0.23076923076923078, 'total': 13, 'correct': 3, 'accuracy_percent': 23.08}, 'moral_disputes': {'accuracy': 0.75, 'total': 12, 'correct': 9, 'accuracy_percent': 75.0}, 'prehistory': {'accuracy': 0.36363636363636365, 'total': 11, 'correct': 4, 'accuracy_percent': 36.36}, 'philosophy': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'high_school_biology': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'nutrition': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'professional_accounting': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'professional_medicine': {'accuracy': 0.3, 'total': 10, 'correct': 3, 'accuracy_percent': 30.0}, 'high_school_mathematics': {'accuracy': 0.3, 'total': 10, 'correct': 3, 'accuracy_percent': 30.0}, 'clinical_knowledge': {'accuracy': 0.8888888888888888, 'total': 9, 'correct': 8, 'accuracy_percent': 88.89}, 'security_studies': {'accuracy': 0.5555555555555556, 'total': 9, 'correct': 5, 'accuracy_percent': 55.56}, 'high_school_microeconomics': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_world_history': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_government_and_politics': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'college_medicine': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'world_religions': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'virology': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'high_school_european_history': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'high_school_physics': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'electrical_engineering': {'accuracy': 0.0, 'total': 5, 'correct': 0, 'accuracy_percent': 0.0}, 'college_biology': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'anatomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'human_sexuality': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'formal_logic': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'public_relations': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'jurisprudence': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'management': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'business_ethics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'high_school_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_chemistry': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_computer_science': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.4397590361445783, 'total': 166, 'correct': 73, 'accuracy_percent': 43.98}, 'Other': {'accuracy': 0.5267857142857143, 'total': 112, 'correct': 59, 'accuracy_percent': 52.68}, 'Social_Sciences': {'accuracy': 0.6090909090909091, 'total': 110, 'correct': 67, 'accuracy_percent': 60.91}, 'STEM': {'accuracy': 0.41964285714285715, 'total': 112, 'correct': 47, 'accuracy_percent': 41.96}}",-25.0
D,"Meta-Llama-3-8B_duplication_[(0,5)]",0.47,500,235,47.0,"{'professional_law': {'accuracy': 0.32727272727272727, 'total': 55, 'correct': 18, 'accuracy_percent': 32.73}, 'moral_scenarios': {'accuracy': 0.25, 'total': 32, 'correct': 8, 'accuracy_percent': 25.0}, 'miscellaneous': {'accuracy': 0.5714285714285714, 'total': 28, 'correct': 16, 'accuracy_percent': 57.14}, 'professional_psychology': {'accuracy': 0.36363636363636365, 'total': 22, 'correct': 8, 'accuracy_percent': 36.36}, 'high_school_psychology': {'accuracy': 0.7368421052631579, 'total': 19, 'correct': 14, 'accuracy_percent': 73.68}, 'high_school_macroeconomics': {'accuracy': 0.5, 'total': 14, 'correct': 7, 'accuracy_percent': 50.0}, 'elementary_mathematics': {'accuracy': 0.15384615384615385, 'total': 13, 'correct': 2, 'accuracy_percent': 15.38}, 'moral_disputes': {'accuracy': 0.5, 'total': 12, 'correct': 6, 'accuracy_percent': 50.0}, 'prehistory': {'accuracy': 0.36363636363636365, 'total': 11, 'correct': 4, 'accuracy_percent': 36.36}, 'philosophy': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'high_school_biology': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'nutrition': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'professional_accounting': {'accuracy': 0.6, 'total': 10, 'correct': 6, 'accuracy_percent': 60.0}, 'professional_medicine': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'high_school_mathematics': {'accuracy': 0.3, 'total': 10, 'correct': 3, 'accuracy_percent': 30.0}, 'clinical_knowledge': {'accuracy': 1.0, 'total': 9, 'correct': 9, 'accuracy_percent': 100.0}, 'security_studies': {'accuracy': 0.4444444444444444, 'total': 9, 'correct': 4, 'accuracy_percent': 44.44}, 'high_school_microeconomics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_world_history': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.875, 'total': 8, 'correct': 7, 'accuracy_percent': 87.5}, 'human_aging': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_statistics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.14285714285714285, 'total': 7, 'correct': 1, 'accuracy_percent': 14.29}, 'sociology': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'high_school_geography': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_government_and_politics': {'accuracy': 0.5714285714285714, 'total': 7, 'correct': 4, 'accuracy_percent': 57.14}, 'college_medicine': {'accuracy': 1.0, 'total': 6, 'correct': 6, 'accuracy_percent': 100.0}, 'world_religions': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'virology': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'high_school_european_history': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'high_school_physics': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'electrical_engineering': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'college_biology': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'anatomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'human_sexuality': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'formal_logic': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'machine_learning': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'public_relations': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'jurisprudence': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'management': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'global_facts': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'business_ethics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'high_school_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_chemistry': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_computer_science': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_mathematics': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.37349397590361444, 'total': 166, 'correct': 62, 'accuracy_percent': 37.35}, 'Other': {'accuracy': 0.6517857142857143, 'total': 112, 'correct': 73, 'accuracy_percent': 65.18}, 'Social_Sciences': {'accuracy': 0.5454545454545454, 'total': 110, 'correct': 60, 'accuracy_percent': 54.55}, 'STEM': {'accuracy': 0.35714285714285715, 'total': 112, 'correct': 40, 'accuracy_percent': 35.71}}",-28.35
F,"Meta-Llama-3-8B_duplication_[(0,4),(27,4)]",0.456,500,228,45.6,"{'professional_law': {'accuracy': 0.34545454545454546, 'total': 55, 'correct': 19, 'accuracy_percent': 34.55}, 'moral_scenarios': {'accuracy': 0.21875, 'total': 32, 'correct': 7, 'accuracy_percent': 21.88}, 'miscellaneous': {'accuracy': 0.5, 'total': 28, 'correct': 14, 'accuracy_percent': 50.0}, 'professional_psychology': {'accuracy': 0.45454545454545453, 'total': 22, 'correct': 10, 'accuracy_percent': 45.45}, 'high_school_psychology': {'accuracy': 0.8421052631578947, 'total': 19, 'correct': 16, 'accuracy_percent': 84.21}, 'high_school_macroeconomics': {'accuracy': 0.42857142857142855, 'total': 14, 'correct': 6, 'accuracy_percent': 42.86}, 'elementary_mathematics': {'accuracy': 0.38461538461538464, 'total': 13, 'correct': 5, 'accuracy_percent': 38.46}, 'moral_disputes': {'accuracy': 0.5833333333333334, 'total': 12, 'correct': 7, 'accuracy_percent': 58.33}, 'prehistory': {'accuracy': 0.2727272727272727, 'total': 11, 'correct': 3, 'accuracy_percent': 27.27}, 'philosophy': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'high_school_biology': {'accuracy': 0.5454545454545454, 'total': 11, 'correct': 6, 'accuracy_percent': 54.55}, 'nutrition': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'professional_accounting': {'accuracy': 0.5, 'total': 10, 'correct': 5, 'accuracy_percent': 50.0}, 'professional_medicine': {'accuracy': 0.4, 'total': 10, 'correct': 4, 'accuracy_percent': 40.0}, 'high_school_mathematics': {'accuracy': 0.2, 'total': 10, 'correct': 2, 'accuracy_percent': 20.0}, 'clinical_knowledge': {'accuracy': 1.0, 'total': 9, 'correct': 9, 'accuracy_percent': 100.0}, 'security_studies': {'accuracy': 0.4444444444444444, 'total': 9, 'correct': 4, 'accuracy_percent': 44.44}, 'high_school_microeconomics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_world_history': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.25, 'total': 8, 'correct': 2, 'accuracy_percent': 25.0}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.14285714285714285, 'total': 7, 'correct': 1, 'accuracy_percent': 14.29}, 'sociology': {'accuracy': 0.42857142857142855, 'total': 7, 'correct': 3, 'accuracy_percent': 42.86}, 'high_school_geography': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_government_and_politics': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'college_medicine': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'world_religions': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'virology': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'high_school_european_history': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'logical_fallacies': {'accuracy': 0.8333333333333334, 'total': 6, 'correct': 5, 'accuracy_percent': 83.33}, 'astronomy': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'high_school_physics': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'electrical_engineering': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'college_biology': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'anatomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'human_sexuality': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'formal_logic': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'machine_learning': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'public_relations': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'jurisprudence': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'management': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'college_physics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'business_ethics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'abstract_algebra': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'medical_genetics': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'high_school_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_chemistry': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_computer_science': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_mathematics': {'accuracy': 1.0, 'total': 3, 'correct': 3, 'accuracy_percent': 100.0}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.3674698795180723, 'total': 166, 'correct': 61, 'accuracy_percent': 36.75}, 'Other': {'accuracy': 0.5892857142857143, 'total': 112, 'correct': 66, 'accuracy_percent': 58.93}, 'Social_Sciences': {'accuracy': 0.5636363636363636, 'total': 110, 'correct': 62, 'accuracy_percent': 56.36}, 'STEM': {'accuracy': 0.3482142857142857, 'total': 112, 'correct': 39, 'accuracy_percent': 34.82}}",-30.49
I,"Meta-Llama-3-8B_duplication_[(0,1),(15,1),(31,1)]",0.454,500,227,45.4,"{'professional_law': {'accuracy': 0.38181818181818183, 'total': 55, 'correct': 21, 'accuracy_percent': 38.18}, 'moral_scenarios': {'accuracy': 0.25, 'total': 32, 'correct': 8, 'accuracy_percent': 25.0}, 'miscellaneous': {'accuracy': 0.5357142857142857, 'total': 28, 'correct': 15, 'accuracy_percent': 53.57}, 'professional_psychology': {'accuracy': 0.5454545454545454, 'total': 22, 'correct': 12, 'accuracy_percent': 54.55}, 'high_school_psychology': {'accuracy': 0.6842105263157895, 'total': 19, 'correct': 13, 'accuracy_percent': 68.42}, 'high_school_macroeconomics': {'accuracy': 0.35714285714285715, 'total': 14, 'correct': 5, 'accuracy_percent': 35.71}, 'elementary_mathematics': {'accuracy': 0.15384615384615385, 'total': 13, 'correct': 2, 'accuracy_percent': 15.38}, 'moral_disputes': {'accuracy': 0.5833333333333334, 'total': 12, 'correct': 7, 'accuracy_percent': 58.33}, 'prehistory': {'accuracy': 0.2727272727272727, 'total': 11, 'correct': 3, 'accuracy_percent': 27.27}, 'philosophy': {'accuracy': 0.7272727272727273, 'total': 11, 'correct': 8, 'accuracy_percent': 72.73}, 'high_school_biology': {'accuracy': 0.6363636363636364, 'total': 11, 'correct': 7, 'accuracy_percent': 63.64}, 'nutrition': {'accuracy': 0.45454545454545453, 'total': 11, 'correct': 5, 'accuracy_percent': 45.45}, 'professional_accounting': {'accuracy': 0.3, 'total': 10, 'correct': 3, 'accuracy_percent': 30.0}, 'professional_medicine': {'accuracy': 0.2, 'total': 10, 'correct': 2, 'accuracy_percent': 20.0}, 'high_school_mathematics': {'accuracy': 0.2, 'total': 10, 'correct': 2, 'accuracy_percent': 20.0}, 'clinical_knowledge': {'accuracy': 0.6666666666666666, 'total': 9, 'correct': 6, 'accuracy_percent': 66.67}, 'security_studies': {'accuracy': 0.3333333333333333, 'total': 9, 'correct': 3, 'accuracy_percent': 33.33}, 'high_school_microeconomics': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'high_school_world_history': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'conceptual_physics': {'accuracy': 0.375, 'total': 8, 'correct': 3, 'accuracy_percent': 37.5}, 'marketing': {'accuracy': 0.75, 'total': 8, 'correct': 6, 'accuracy_percent': 75.0}, 'human_aging': {'accuracy': 0.625, 'total': 8, 'correct': 5, 'accuracy_percent': 62.5}, 'high_school_statistics': {'accuracy': 0.5, 'total': 8, 'correct': 4, 'accuracy_percent': 50.0}, 'high_school_us_history': {'accuracy': 0.0, 'total': 7, 'correct': 0, 'accuracy_percent': 0.0}, 'high_school_chemistry': {'accuracy': 0.2857142857142857, 'total': 7, 'correct': 2, 'accuracy_percent': 28.57}, 'sociology': {'accuracy': 0.7142857142857143, 'total': 7, 'correct': 5, 'accuracy_percent': 71.43}, 'high_school_geography': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'high_school_government_and_politics': {'accuracy': 0.8571428571428571, 'total': 7, 'correct': 6, 'accuracy_percent': 85.71}, 'college_medicine': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'world_religions': {'accuracy': 0.6666666666666666, 'total': 6, 'correct': 4, 'accuracy_percent': 66.67}, 'virology': {'accuracy': 0.5, 'total': 6, 'correct': 3, 'accuracy_percent': 50.0}, 'high_school_european_history': {'accuracy': 0.3333333333333333, 'total': 6, 'correct': 2, 'accuracy_percent': 33.33}, 'logical_fallacies': {'accuracy': 1.0, 'total': 6, 'correct': 6, 'accuracy_percent': 100.0}, 'astronomy': {'accuracy': 0.4, 'total': 5, 'correct': 2, 'accuracy_percent': 40.0}, 'high_school_physics': {'accuracy': 0.2, 'total': 5, 'correct': 1, 'accuracy_percent': 20.0}, 'electrical_engineering': {'accuracy': 0.0, 'total': 5, 'correct': 0, 'accuracy_percent': 0.0}, 'college_biology': {'accuracy': 0.8, 'total': 5, 'correct': 4, 'accuracy_percent': 80.0}, 'anatomy': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'human_sexuality': {'accuracy': 0.6, 'total': 5, 'correct': 3, 'accuracy_percent': 60.0}, 'formal_logic': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'international_law': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'econometrics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'machine_learning': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'public_relations': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'jurisprudence': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'management': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'college_physics': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'us_foreign_policy': {'accuracy': 0.75, 'total': 4, 'correct': 3, 'accuracy_percent': 75.0}, 'global_facts': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'business_ethics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'abstract_algebra': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'medical_genetics': {'accuracy': 0.5, 'total': 4, 'correct': 2, 'accuracy_percent': 50.0}, 'high_school_computer_science': {'accuracy': 0.25, 'total': 4, 'correct': 1, 'accuracy_percent': 25.0}, 'college_chemistry': {'accuracy': 0.0, 'total': 4, 'correct': 0, 'accuracy_percent': 0.0}, 'college_computer_science': {'accuracy': 1.0, 'total': 4, 'correct': 4, 'accuracy_percent': 100.0}, 'college_mathematics': {'accuracy': 0.0, 'total': 3, 'correct': 0, 'accuracy_percent': 0.0}, 'computer_security': {'accuracy': 0.3333333333333333, 'total': 3, 'correct': 1, 'accuracy_percent': 33.33}}","{'Humanities': {'accuracy': 0.40963855421686746, 'total': 166, 'correct': 68, 'accuracy_percent': 40.96}, 'Other': {'accuracy': 0.49107142857142855, 'total': 112, 'correct': 55, 'accuracy_percent': 49.11}, 'Social_Sciences': {'accuracy': 0.5818181818181818, 'total': 110, 'correct': 64, 'accuracy_percent': 58.18}, 'STEM': {'accuracy': 0.35714285714285715, 'total': 112, 'correct': 40, 'accuracy_percent': 35.71}}",-30.79
